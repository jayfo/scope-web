{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61593d0f-147b-4e8a-beaf-cd976cc6baee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Patient Data Export\n",
    "\n",
    "A notebook exporting patient data for study analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e6585-0e45-4a7c-9b1a-69def1590cb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a95cf5-b99d-4ea9-8103-c05da885e137",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a14340-9b0a-463e-a054-b5229a570e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import bson.objectid\n",
    "import IPython.display\n",
    "import ipywidgets\n",
    "import nbformat\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import pyzipper\n",
    "import scope.database.date_utils as date_utils\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n",
    "from scope.documents import document_set\n",
    "from scope.populate.data.archive import Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286d4c9-06f8-4068-9ef7-71951a155c1b",
   "metadata": {},
   "source": [
    "### Constants and Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e556573-c100-4716-8884-af2539d96de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In development, it can be helpful to sample a subset of patients.\n",
    "# If DEVELOPMENT_SAMPLE_PATIENTS <= 0, process all patients.\n",
    "# If DEVELOPMENT_SAMPLE_PATIENTS > 0, randomly sample DEVELOPMENT_SAMPLE_PATIENTS patients.\n",
    "DEVELOPMENT_SAMPLE_PATIENTS: int = -1\n",
    "\n",
    "# In development, it can be helpful to skip per-patient export.\n",
    "# If DEVELOPMENT_EXPORT_PER_PATIENT_DOCUMENTS, include per-patient export.\n",
    "DEVELOPMENT_EXPORT_PER_PATIENT_DOCUMENTS: bool = False\n",
    "\n",
    "# In development, it can be helpful to skip documents export.\n",
    "# If DEVELOPMENT_EXPORT_COMBINED_DOCUMENTS, include documents export.\n",
    "DEVELOPMENT_EXPORT_COMBINED_DOCUMENTS: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5e56d-fe30-4482-a286-e354f56089f5",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99409d53-ba6c-41af-9709-9bc14f428bdd",
   "metadata": {},
   "source": [
    "### Utility: documentation_as_markdown\n",
    "\n",
    "Returns a string containing markdown content recovered from a cell in this notebook.\n",
    "\n",
    "Intended to allow including the content of markdown cells as documentation in an export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed34a35-5424-43fc-91a7-75c1de3eace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentation_as_markdown(documentation_name: str) -> str:\n",
    "    # Load this same notebook.\n",
    "    notebook = nbformat.read(\"patientdata.ipynb\", nbformat.NO_CONVERT)\n",
    "\n",
    "    # Go through each cell, looking for a match.\n",
    "    for cell_current in notebook[\"cells\"]:\n",
    "        match = True\n",
    "        if match:\n",
    "            match = cell_current[\"cell_type\"] == \"markdown\"\n",
    "        if match:\n",
    "            match = re.match(\n",
    "                \"^(#*) Documentation: ({})\\\\n(.*)\".format(documentation_name),\n",
    "                cell_current[\"source\"],\n",
    "            )\n",
    "\n",
    "        if match:\n",
    "            return cell_current[\"source\"]\n",
    "\n",
    "    # If no match was found, raise a ValueError.\n",
    "    raise ValueError(\n",
    "        \"No matching documentation cell found: {}\".format(documentation_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e33a0-8776-4709-9bfe-e7f3e64e5556",
   "metadata": {},
   "source": [
    "### Utility: ExportFile\n",
    "\n",
    "The path and contents of a file to be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545241d-24f5-46c1-83ba-d393f27e4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportFileType(Enum):\n",
    "    BYTES = \"BYTES\"\n",
    "    CSV = \"CSV\"\n",
    "    EXCEL = \"EXCEL\"\n",
    "    MARKDOWN = \"MARKDOWN\"\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ExportFile:\n",
    "    path: pathlib.Path\n",
    "    type: ExportFileType\n",
    "    bytes: Optional[bytes]\n",
    "    text: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79278bd-fcb9-429b-a80d-7ba7f55bd101",
   "metadata": {},
   "source": [
    "### Utility: export_dataframe_as_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95052fe4-a91c-4c5e-9c07-a5f255849f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe_as_csv(\n",
    "    path: pathlib.Path,\n",
    "    df: pd.DataFrame,\n",
    "):\n",
    "    path = path.with_suffix(path.suffix + \".csv\")\n",
    "\n",
    "    csv_text = df.to_csv(index=False)\n",
    "\n",
    "    export_file_list.append(\n",
    "        ExportFile(\n",
    "            path=path,\n",
    "            type=ExportFileType.CSV,\n",
    "            bytes=None,\n",
    "            text=csv_text,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d60c8-3744-4fb3-a62b-a66c79c4f6c9",
   "metadata": {},
   "source": [
    "### Utility: export_dataframe_as_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cc34b-cb6c-4a59-8a8d-f203da12e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe_as_excel(\n",
    "    path: pathlib.Path,\n",
    "    df: pd.DataFrame,\n",
    "):\n",
    "    path = path.with_suffix(path.suffix + \".xlsx\")\n",
    "\n",
    "    iobytes = io.BytesIO()\n",
    "    df.to_excel(iobytes, index=False)\n",
    "    excel_bytes = iobytes.getvalue()\n",
    "\n",
    "    export_file_list.append(\n",
    "        ExportFile(\n",
    "            path=path,\n",
    "            type=ExportFileType.EXCEL,\n",
    "            bytes=excel_bytes,\n",
    "            text=None,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1a386-7153-49b8-8f5e-43bd0c844bed",
   "metadata": {},
   "source": [
    "### Utility: export_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144d5cf-1701-40b2-9883-2108c53cfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe(\n",
    "    path: pathlib.Path,\n",
    "    df: pd.DataFrame,\n",
    "):\n",
    "    export_dataframe_as_csv(path, df)\n",
    "    export_dataframe_as_excel(path, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519e6d6-effb-49ca-af9b-633a917334ac",
   "metadata": {},
   "source": [
    "### Utility: export_file_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb23304-edd7-4343-b462-a788928c4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_file_bytes(\n",
    "    path: pathlib.Path,\n",
    "    file_bytes: bytes,\n",
    "):\n",
    "    export_file_list.append(\n",
    "        ExportFile(\n",
    "            path=path,\n",
    "            type=ExportFileType.BYTES,\n",
    "            bytes=file_bytes,\n",
    "            text=None,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c2694-ec38-4e7f-a00f-1d0682ac8793",
   "metadata": {},
   "source": [
    "### Utility: export_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541f7f1-56ec-4d8a-8b3c-a70a7f909f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_markdown(\n",
    "    path: pathlib.Path,\n",
    "    markdown: str,\n",
    "):\n",
    "    path = path.with_suffix(path.suffix + \".md\")\n",
    "\n",
    "    export_file_list.append(\n",
    "        ExportFile(\n",
    "            path=path,\n",
    "            type=ExportFileType.MARKDOWN,\n",
    "            bytes=None,\n",
    "            text=markdown,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778098a-f807-48e4-b6ee-8323da24dfcd",
   "metadata": {},
   "source": [
    "### Utility: dataframe_sanitize\n",
    "\n",
    "Sanitize contents of a dataframe that otherwise cannot be written to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf9fae-1ce8-4251-9cb0-209faa666ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_sanitize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def sanitize_cell(value):\n",
    "        if type(value) == str:\n",
    "            value = ILLEGAL_CHARACTERS_RE.sub(\"?\", value)\n",
    "\n",
    "        return value\n",
    "\n",
    "    return df.map(sanitize_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febd58c-90b6-4600-99f6-f01d3409dbb6",
   "metadata": {},
   "source": [
    "### Utility: dataframe_format_export\n",
    "\n",
    "Formats a dataframe for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd1555-c7b8-4e2c-acf8-ad7c06eeddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_format_export(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    drop_empty_columns: Optional[bool] = False,\n",
    "    drop_columns: Optional[List[str]] = None,\n",
    "    rename_columns: Optional[Dict[str, str]] = None,\n",
    "    sort_columns: Optional[List[str]] = None,\n",
    "    sort_rows_by_columns: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    # Ensure we are modifying a copy.\n",
    "    df = df.copy()\n",
    "\n",
    "    # If requested, drop empty columns.\n",
    "    if drop_empty_columns:\n",
    "        empty_columns = []\n",
    "        for column_current in df.columns:\n",
    "            if (\n",
    "                df[column_current].isnull().all()\n",
    "                or (\n",
    "                    df[column_current].astype(str).str.strip().isin([\"\", \"nan\", \"None\"])\n",
    "                ).all()\n",
    "            ):\n",
    "                empty_columns.append(column_current)\n",
    "\n",
    "        df = df.drop(columns=empty_columns)\n",
    "\n",
    "    # If requested, drop specific columns.\n",
    "    # Be robust to the possibility that a column is not present.\n",
    "    if drop_columns:\n",
    "        df = df.drop(columns=drop_columns, errors=\"ignore\")\n",
    "\n",
    "    # If requested, rename specific columns.\n",
    "    if rename_columns:\n",
    "        df = df.rename(columns=rename_columns)\n",
    "\n",
    "    # If requested, sort specific columns to the front.\n",
    "    # Be robust to the possibility that a column is not present.\n",
    "    # Be robust to the presence of additional columns.\n",
    "    # Preserve existing order of additional columns after requested columns.\n",
    "    if sort_columns:\n",
    "        sort_columns = [\n",
    "            column_current\n",
    "            for column_current in sort_columns\n",
    "            if column_current in df.columns\n",
    "        ]\n",
    "        sort_columns = sort_columns + [\n",
    "            column_current\n",
    "            for column_current in df.columns\n",
    "            if column_current not in sort_columns\n",
    "        ]\n",
    "\n",
    "        df = df[sort_columns]\n",
    "\n",
    "    # If requested, sort rows by specific columns.\n",
    "    # Be robust to the possibility that a column is not present.\n",
    "    if sort_rows_by_columns:\n",
    "        sort_rows_by_columns = [\n",
    "            column_current\n",
    "            for column_current in sort_rows_by_columns\n",
    "            if column_current in df.columns\n",
    "        ]\n",
    "\n",
    "        df = df.sort_values(sort_rows_by_columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb2af0-4ce5-46e3-adc7-2b9c7d2fe10e",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc219c-78b4-4d41-b69e-97b28867faa5",
   "metadata": {},
   "source": [
    "### Utility: archive_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9820e0-810c-451b-aabb-8d8782de1b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path containing encrypted archives.\n",
    "archive_dir_path = \"../../../secrets/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6a8f0-57ed-4d91-a97e-f21652c495d6",
   "metadata": {},
   "source": [
    "### Input Archive Suffix: archive_suffix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42980a3-e834-44d8-8737-cdfdfa562b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain suffix indicating desired version of encrypted archives.\n",
    "# Do not include the '.zip' suffix.\n",
    "def input_archive_suffix():\n",
    "    # Based on archives in our path, identify possible suffixes.\n",
    "    pattern = re.compile(\"archive_(multicare|scca)_([^_]+)_(\\\\d{8})(?:_(.+))?.zip\")\n",
    "    archive_file_names = [\n",
    "        name_current\n",
    "        for name_current in os.listdir(archive_dir_path)\n",
    "        if pattern.search(name_current)\n",
    "    ]\n",
    "    archive_matches = [\n",
    "        re.match(pattern, name_current) for name_current in archive_file_names\n",
    "    ]\n",
    "    archive_suffixes = list(\n",
    "        {\n",
    "            \"{}_{}{}\".format(\n",
    "                match_current.group(2),\n",
    "                match_current.group(3),\n",
    "                \"_\" + match_current.group(4) if len(match_current.groups()) > 3 else \"\",\n",
    "            )\n",
    "            for match_current in archive_matches\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # If there is only one possible value, no need for a choice.\n",
    "    archive_suffix = None\n",
    "    if len(archive_suffixes) == 1:\n",
    "        archive_suffix = archive_suffixes[0]\n",
    "    else:\n",
    "        for id_current, suffix_current in enumerate(archive_suffixes, 1):\n",
    "            print(\"[{}]: {}\".format(id_current, suffix_current))\n",
    "\n",
    "        archive_suffix = archive_suffixes[\n",
    "            int(input(\"Encrypted archive suffix index: \")) - 1\n",
    "        ]\n",
    "\n",
    "    print(archive_suffix)\n",
    "\n",
    "    return archive_suffix\n",
    "\n",
    "\n",
    "archive_suffix = input_archive_suffix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c182cd-b385-4251-8c92-32bd4f985b44",
   "metadata": {},
   "source": [
    "### Input Archive Password: archive_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7154a-6053-44d7-b6f4-504cfc4d0d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain password to encrypted archives.\n",
    "archive_password = input(\"Encrypted archive password: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff265d-ed78-49b9-981f-96aff3c79488",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51623c25-8317-489a-ae4d-579e97684a21",
   "metadata": {},
   "source": [
    "### Decrypt Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920047a-b3e7-4353-9e6a-fe1000ccc854",
   "metadata": {},
   "source": [
    "#### Data: archive_multicare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b7997-4ec7-4467-b8aa-3b18155161e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decrypt_archive_multicare():\n",
    "    # Obtain name for each archive.\n",
    "    archive_multicare_file_name = \"archive_multicare_{}.zip\".format(archive_suffix)\n",
    "\n",
    "    # Obtain a full path to encrypted archive, relative to the location of the notebook.\n",
    "    # Expects the encrypted archive to be in the \"secrets/data\" directory.\n",
    "    archive_multicare_path = pathlib.Path(\n",
    "        archive_dir_path,\n",
    "        archive_multicare_file_name,\n",
    "    )\n",
    "\n",
    "    print(\"Decrypting archive:\")\n",
    "    print(\"{}\".format(archive_multicare_path.resolve()))\n",
    "\n",
    "    # Obtain the archive.\n",
    "    archive_multicare = Archive.read_archive(\n",
    "        archive_path=archive_multicare_path,\n",
    "        password=archive_password,\n",
    "    )\n",
    "\n",
    "    print(\"{} documents.\".format(len(archive_multicare.entries.values())))\n",
    "\n",
    "    return archive_multicare\n",
    "\n",
    "\n",
    "archive_multicare = decrypt_archive_multicare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144d281-ccd5-45fa-8b5d-8517b3146e5b",
   "metadata": {},
   "source": [
    "#### Data: archive_scca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc41dd1-4a40-4256-8ba8-8c34486c1021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decrypt_archive_scca():\n",
    "    # Obtain name for each archive.\n",
    "    archive_scca_file_name = \"archive_scca_{}.zip\".format(archive_suffix)\n",
    "\n",
    "    # Obtain a full path to encrypted archive, relative to the location of the notebook.\n",
    "    # Expects the encrypted archive to be in the \"secrets/data\" directory.\n",
    "    archive_scca_path = pathlib.Path(\n",
    "        archive_dir_path,\n",
    "        archive_scca_file_name,\n",
    "    )\n",
    "\n",
    "    print(\"Decrypting archive:\")\n",
    "    print(\"{}\".format(archive_scca_path.resolve()))\n",
    "\n",
    "    # Obtain the archive.\n",
    "    archive_scca = Archive.read_archive(\n",
    "        archive_path=archive_scca_path,\n",
    "        password=archive_password,\n",
    "    )\n",
    "\n",
    "    print(\"{} documents.\".format(len(archive_scca.entries.values())))\n",
    "\n",
    "    return archive_scca\n",
    "\n",
    "\n",
    "archive_scca = decrypt_archive_scca()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263b536-346d-42f4-87ba-bbe6b30c651b",
   "metadata": {},
   "source": [
    "### Decrypt MRN to RecordId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33b584-0012-48f7-af66-d81249e289b5",
   "metadata": {},
   "source": [
    "#### Data: mrn_to_record_id_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148ce38-581d-42de-a3a5-efb919c422ce",
   "metadata": {},
   "source": [
    "#### Data: mrn_to_record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b03303-0595-4e77-9226-a49e73247027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decrypt_mrn_to_record_id():\n",
    "    # Obtain a full path to encrypted archive, relative to the location of the notebook.\n",
    "    # Expects the encrypted archive to be in the \"secrets/data\" directory.\n",
    "    archive_mrn_to_record_id_path = pathlib.Path(\n",
    "        archive_dir_path,\n",
    "        \"archive_mrn_to_record_id.zip\",\n",
    "    )\n",
    "\n",
    "    # Open the file\n",
    "    with open(\n",
    "        archive_mrn_to_record_id_path,\n",
    "        mode=\"rb\",\n",
    "    ) as archive_file:\n",
    "        with pyzipper.AESZipFile(\n",
    "            archive_file,\n",
    "            \"r\",\n",
    "            compression=pyzipper.ZIP_LZMA,\n",
    "            encryption=pyzipper.WZ_AES,\n",
    "        ) as archive_zipfile:\n",
    "            # Set the zipfile password\n",
    "            archive_zipfile.setpassword(archive_password.encode(\"utf-8\"))\n",
    "\n",
    "            # Confirm the zipfile is valid\n",
    "            if archive_zipfile.testzip():\n",
    "                raise ValueError(\"Invalid archive or password\")\n",
    "\n",
    "            # Retrieve the Excel file.\n",
    "            excel_bytes = archive_zipfile.read(\n",
    "                \"archive_mrn_to_record_id/archive_mrn_to_record_id.xlsx\"\n",
    "            )\n",
    "            df_mrn_to_record_id = pd.read_excel(io.BytesIO(excel_bytes))\n",
    "\n",
    "            # Ensure MRN are treated as strings.\n",
    "            df_mrn_to_record_id[\"MRN\"] = df_mrn_to_record_id[\"MRN\"].astype(str)\n",
    "\n",
    "            # Return the Excel file and a dictionary.\n",
    "            return (\n",
    "                excel_bytes,\n",
    "                df_mrn_to_record_id.set_index(\"MRN\")[\"recordId\"].to_dict(),\n",
    "            )\n",
    "\n",
    "\n",
    "mrn_to_record_id_bytes, mrn_to_record_id = decrypt_mrn_to_record_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f73156-10d5-4019-970c-f02ea8b8dd5b",
   "metadata": {},
   "source": [
    "## Process Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f38ca9-122f-4d40-8270-69c3964418ea",
   "metadata": {},
   "source": [
    "### Combine Patients DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e6366-e7b0-4f3c-812b-033905a4cc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_patients_dataframes():\n",
    "    # Get patient documents from MultiCare.\n",
    "    documents_multicare_patients = (\n",
    "        archive_multicare.collection_documents(\n",
    "            collection=\"patients\",\n",
    "        )\n",
    "        .remove_sentinel()\n",
    "        .remove_revisions()\n",
    "    )\n",
    "    df_multicare_patients = pd.DataFrame.from_records(\n",
    "        documents_multicare_patients.documents\n",
    "    )\n",
    "    df_multicare_patients[\"database\"] = \"multicare\"\n",
    "\n",
    "    # Get patient documents from SCCA.\n",
    "    documents_scca_patients = (\n",
    "        archive_scca.collection_documents(\n",
    "            collection=\"patients\",\n",
    "        )\n",
    "        .remove_sentinel()\n",
    "        .remove_revisions()\n",
    "    )\n",
    "    df_scca_patients = pd.DataFrame.from_records(documents_scca_patients.documents)\n",
    "    df_scca_patients[\"database\"] = \"fhcc\"\n",
    "\n",
    "    # Unify all current patient documents.\n",
    "    df_combined_patients = pd.concat(\n",
    "        [df_multicare_patients, df_scca_patients]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Sanitize once so contents dataframe can be exported.\n",
    "    df_combined_patients = dataframe_sanitize(df_combined_patients)\n",
    "\n",
    "    return df_combined_patients\n",
    "\n",
    "\n",
    "df_archive_patients_raw = combine_patients_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7038f7-35aa-4cdd-9024-e0247c118838",
   "metadata": {},
   "source": [
    "### Reset Filtering and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126106ee-d568-48ad-b416-361f0a4442ee",
   "metadata": {},
   "source": [
    "#### Data: df_archive_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf30078-c24d-462b-bdcf-8295205810c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_archive_patients = df_archive_patients_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7526ef-f9fb-4419-90c7-4c67e26e3e2d",
   "metadata": {},
   "source": [
    "### Filter Pilot Patients\n",
    "\n",
    "Remove the 6 pilot patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f432d-8e6b-4039-97da-fc2d9bef3dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_archive_patients = df_archive_patients.drop(\n",
    "    df_archive_patients[\n",
    "        df_archive_patients[\"patientId\"].isin(\n",
    "            [\n",
    "                \"ymzwx6e6w6kqi\",\n",
    "                \"mmmb54v52l7re\",\n",
    "                \"ouoa4ucldbhie\",\n",
    "                \"zazst4yu23a5q\",\n",
    "                \"wf4btxqjtd2oa\",\n",
    "                \"s3bcmgmp7gdss\",\n",
    "            ]\n",
    "        )\n",
    "    ].index\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb113eeb-81af-4e9f-883c-82b4e2d9264f",
   "metadata": {},
   "source": [
    "### Sample Patients\n",
    "\n",
    "In development, it can be helpful to sample a subset of patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8ff65-aab7-45ab-9a0d-03ec38e54e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVELOPMENT_SAMPLE_PATIENTS > 0:\n",
    "    df_archive_patients = df_archive_patients.sample(\n",
    "        n=DEVELOPMENT_SAMPLE_PATIENTS\n",
    "    ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfcc533-1006-466d-aeee-d205cfbb2308",
   "metadata": {},
   "source": [
    "### Utility: patient_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1dec2-f5aa-4869-8fa0-0656e9233426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a helper for accessing the document collection of a unified patient.\n",
    "def patient_documents(row_patient) -> document_set.DocumentSet:\n",
    "    if row_patient[\"database\"] == \"multicare\":\n",
    "        archive = archive_multicare\n",
    "    elif row_patient[\"database\"] == \"fhcc\":\n",
    "        archive = archive_scca\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    return archive.collection_documents(collection=row_patient[\"collection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b46616-323b-44e0-a8bd-823bc56d08cb",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0294d90-f988-4641-89b7-bf71b035a3a0",
   "metadata": {},
   "source": [
    "### Prepare Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43060c1-9fde-4d5b-a58e-34d8d32dffe0",
   "metadata": {},
   "source": [
    "#### Documentation: Patients\n",
    "\n",
    "All patients included in this export, based on `patientIdentity` documents.\n",
    "These are stored separately from the collection of documents associated with each patient.\n",
    "They provide an index of all patients, then we can access the collection of documents for each individual patient.\n",
    "\n",
    "A `patientIdentity` document may have been modified throughout the study (e.g., to change a patient name or email address).\n",
    "If the document was modified, this export includes only the final version.\n",
    "There is therefore exactly one row per patient.\n",
    "\n",
    "Data is originally taken from two database exports: from FHCC and from MultiCare.\n",
    "Raw data are merged, then a `database` column is added to indicate the origin of each patient.\n",
    "\n",
    "There were 6 pilot patients. These have been completely removed.\n",
    "\n",
    "TODO: The patient with `recordId` `1747` maps to two different `MRN` and `patientId`.\n",
    "This was because they moved from one system to another during the study.\n",
    "There is still a need to decide how to special case that patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e3eb78-48da-4593-aa19-39c0c4c39956",
   "metadata": {},
   "source": [
    "#### Data: df_patients_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88901c3d-30ef-4f1f-95ea-0635018f4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients_raw = df_archive_patients.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e604e-3f76-4d03-bf94-40f8b1e67286",
   "metadata": {},
   "source": [
    "#### Data: df_patients\n",
    "\n",
    "Remove most fields, so they are not needlessly visible during export script development.\n",
    "\n",
    "Use `MRN` to map each patient to a `recordId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6efb1-2552-4b66-b550-f4bc922f08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients_raw.copy()\n",
    "\n",
    "# Ensure MRN are treated as strings.\n",
    "df_patients[\"MRN\"] = df_patients[\"MRN\"].astype(str)\n",
    "\n",
    "# Apply a transform to look up the recordId.\n",
    "def transform_add_record_id(\n",
    "    df_patients: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    def _transform_add_record_id_from_mrn(row):\n",
    "        return str(mrn_to_record_id.get(row[\"MRN\"], \"???\"))\n",
    "\n",
    "    df_patients = df_patients.copy()\n",
    "    df_patients[\"recordId\"] = df_patients.apply(\n",
    "        _transform_add_record_id_from_mrn, axis=1\n",
    "    )\n",
    "\n",
    "    return df_patients\n",
    "\n",
    "\n",
    "df_patients = transform_add_record_id(df_patients)\n",
    "\n",
    "# \"collection\" cannot be removed at this point, as it is used by later processing.\n",
    "df_patients = dataframe_format_export(\n",
    "    df_patients,\n",
    "    drop_columns=[\n",
    "        # Remove clutter.\n",
    "        \"_id\",\n",
    "        \"_rev\",\n",
    "        \"_set_id\",\n",
    "        \"_type\",\n",
    "        # Remove identifiers.\n",
    "        \"cognitoAccount\",  # Includes email.\n",
    "        \"name\",\n",
    "    ],\n",
    "    sort_columns=[\n",
    "        \"recordId\",\n",
    "        \"database\",\n",
    "        \"patientId\",\n",
    "        \"MRN\",\n",
    "        \"collection\",\n",
    "    ],\n",
    "    sort_rows_by_columns=[\n",
    "        \"recordId\",\n",
    "        \"database\",\n",
    "        \"patientId\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907b9e9-94ae-4c91-b795-3fdb34b15e7c",
   "metadata": {},
   "source": [
    "#### Data: patient_id_to_record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1179cb-a945-4fd5-8620-f18216e5073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id_to_record_id = (\n",
    "    df_patients.copy().set_index(\"patientId\")[\"recordId\"].to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77f0b0-a958-4d59-b51e-f0595e9f4b58",
   "metadata": {},
   "source": [
    "### Prepare Per-Patient DocumentSets and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606174cc-fbc4-4a54-b410-ddca1387cf82",
   "metadata": {},
   "source": [
    "#### Data: patient_id_to_documentset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee2a76-b95b-4c69-9734-94abaf7badf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_patient_id_to_documentset():\n",
    "    patient_id_to_documentset = {}\n",
    "\n",
    "    progress_max = len(df_patients)\n",
    "    progress_patient_count = ipywidgets.IntProgress(min=0, max=progress_max)\n",
    "\n",
    "    print(\"Preparing DocumentSet for {} Patients\".format(progress_max))\n",
    "    IPython.display.display(progress_patient_count)\n",
    "\n",
    "    progress_patient_count.description = \"{}/{}\".format(0, progress_max)\n",
    "    for patient_count, (row_current, patient_current) in enumerate(\n",
    "        df_patients.iterrows()\n",
    "    ):\n",
    "        patient_id_current = patient_current[\"patientId\"]\n",
    "        patient_collection = patient_documents(patient_current.to_dict())\n",
    "\n",
    "        patient_id_to_documentset[patient_id_current] = patient_collection\n",
    "\n",
    "        progress_patient_count.description = \"{}/{}\".format(\n",
    "            patient_count + 1, progress_max\n",
    "        )\n",
    "        progress_patient_count.value = patient_count + 1\n",
    "\n",
    "    return patient_id_to_documentset\n",
    "\n",
    "\n",
    "patient_id_to_documentset = prepare_patient_id_to_documentset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86fcbb-f16c-4a82-b4bb-250aafb8692a",
   "metadata": {},
   "source": [
    "#### Transform: transform_add_created\n",
    "\n",
    "Add a `_created` column to each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8941d-7f54-44d8-bb78-3b6b7f558d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_add_created(\n",
    "    df_documents: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    def _transform_add_created_from_id(row):\n",
    "        datetime_parsed = bson.objectid.ObjectId(row[\"_id\"]).generation_time.astimezone(\n",
    "            pytz.utc\n",
    "        )\n",
    "        datetime_pacific = datetime_parsed.astimezone(\n",
    "            pytz.timezone(\"America/Los_Angeles\")\n",
    "        )\n",
    "\n",
    "        return datetime_pacific.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    df_documents = df_documents.copy()\n",
    "    df_documents[\"_created\"] = df_documents.apply(\n",
    "        _transform_add_created_from_id, axis=1\n",
    "    )\n",
    "\n",
    "    return df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b0f789-9ef1-4bb8-89d8-12768e9a3ae7",
   "metadata": {},
   "source": [
    "#### Transform: transform_add_record_id_and_patient_id\n",
    "\n",
    "Add a `patientId` column to each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480dd19b-b39d-4497-aeae-4163ab11b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_add_record_id_and_patient_id(\n",
    "    df_documents: pd.DataFrame,\n",
    "    *,\n",
    "    patient_id,\n",
    ") -> pd.DataFrame:\n",
    "    df_documents = df_documents.copy()\n",
    "    df_documents[\"recordId\"] = patient_id_to_record_id[patient_id]\n",
    "    df_documents[\"_patientId\"] = patient_id\n",
    "\n",
    "    return df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4ee33-c599-4ad1-b283-50822f08f533",
   "metadata": {},
   "source": [
    "#### Data: patient_id_to_df_documents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b988c74-ea09-456b-811b-ff2287b0d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_patient_id_to_df_documents_raw():\n",
    "    patient_id_to_df_documents_raw = {}\n",
    "\n",
    "    progress_max = len(patient_id_to_documentset)\n",
    "    progress_patient_count = ipywidgets.IntProgress(min=0, max=progress_max)\n",
    "\n",
    "    print(\"Preparing DataFrame for {} Patients\".format(progress_max))\n",
    "    IPython.display.display(progress_patient_count)\n",
    "\n",
    "    progress_patient_count.description = \"{}/{}\".format(0, progress_max)\n",
    "    for patient_count, (patient_id_current, patient_documentset_current) in enumerate(\n",
    "        patient_id_to_documentset.items()\n",
    "    ):\n",
    "        df_documents_raw_current = pd.DataFrame.from_records(\n",
    "            patient_documentset_current.documents\n",
    "        )\n",
    "\n",
    "        # Sanitize contents for export.\n",
    "        df_documents_raw_current = dataframe_sanitize(df_documents_raw_current)\n",
    "\n",
    "        # Apply minimal transforms to the \"raw\" documents.\n",
    "        df_documents_raw_current = transform_add_created(\n",
    "            df_documents_raw_current,\n",
    "        )\n",
    "        df_documents_raw_current = transform_add_record_id_and_patient_id(\n",
    "            df_documents_raw_current, patient_id=patient_id_current\n",
    "        )\n",
    "\n",
    "        patient_id_to_df_documents_raw[patient_id_current] = df_documents_raw_current\n",
    "\n",
    "        progress_patient_count.description = \"{}/{}\".format(\n",
    "            patient_count + 1, progress_max\n",
    "        )\n",
    "        progress_patient_count.value = patient_count + 1\n",
    "\n",
    "    return patient_id_to_df_documents_raw\n",
    "\n",
    "\n",
    "patient_id_to_df_documents_raw = prepare_patient_id_to_df_documents_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d598e15-3a4c-4d18-bd1b-2a0c6c78c385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_documents_raw = pd.concat(patient_id_to_df_documents_raw.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9244cc-a28e-4fb7-aa1c-c4b771ae85f5",
   "metadata": {},
   "source": [
    "## Transform Documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dca9f004-1722-45b5-b6b4-e8d629c24de9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Documentation: Common Fields\n",
    "\n",
    "Several common fields are shared across different analysis exports:\n",
    "\n",
    "- `recordId` identifies the patient.\n",
    "\n",
    "- `_patientId` and `_docId` uniquely identify a document.\n",
    "  These are intended primarily for inspection and data cleaning, and are not intended to be used in analyses.\n",
    "\n",
    "- If there could be multiple instances of a document type (e.g., multiple assessment logs, multiple mood logs),\n",
    "  then documents include a type-specific identifier (e.g., `_assessmentLogId`, `_moodLogId`).\n",
    "\n",
    "- `_rev` identifies the version of the document (i.e., the revision).\n",
    "  If only one instance of a document type is allowed (e.g., a single safety plan),\n",
    "  this identifies revisions of that document over time.\n",
    "  If there could be multiple instances of a document type,\n",
    "  the combination of the type-specific identifier and the version identifies revisions of specific instances.\n",
    "\n",
    "- `_created` indicates when a document was created, recovered from an encoding within `_docId`.\n",
    "  This is provided in Pacific time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9a7ee79-e243-4cfd-bc68-7044b5d01aa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Documentation: Assessments\n",
    "\n",
    "Exported from `assessmentLog` documents. A single row is included for each `assessmentLog`.\n",
    "\n",
    "Includes common fields documented in `commonFields.md`.\n",
    "\n",
    "Notable values:\n",
    "\n",
    "- `assessmentId` will be either `gad-7` or `phq-9`.\n",
    "\n",
    "- Documents include both `_created` and `recordedDate`.\n",
    "\n",
    "  - Per commond fields, `_created` indicates when a document was created.\n",
    "\n",
    "  - `recordedDate` indicates what date was indicated for an `assessmentLog`.\n",
    "\n",
    "  - For assessment logs submitted via the patient app, these were the same.\n",
    "\n",
    "  - For assessment logs submitted via the provider registry, the provider entered the date of the assessment log.\n",
    "\n",
    "Notable transformations:\n",
    "\n",
    "- `recordedDate` is calculated in Pacific time from a raw `recordedDateTime`.\n",
    "\n",
    "- `submittedBy` is calculated from a raw `patientSubmitted`.\n",
    "\n",
    "- If individual scale components were available in a raw `pointValues`,\n",
    "  they were promoted to columns (e.g., `gad7Anxious`, `phq9Interest`).\n",
    "\n",
    "- An assessment score (i.e., `gad7Score`, `phq9Score`)\n",
    "  was taken from a raw `totalScore` or by summing the values in a raw `pointValues`.\n",
    "\n",
    "TODO: Inspecting a sample of revised assessment logs suggests there will not be a simple approach to data cleaning.\n",
    "\n",
    "TODO: `submittedBy` value of `SW` is potentially misleading.\n",
    "These were created using the registry, but not necessarily by a social worker.\n",
    "\n",
    "TODO: `todo_scheduledAssessmentId` is present for patient-submitted entries, may allow recovering information about an assessment schedule.\n",
    "It is unclear how correct this field is, so it may be best to not rely upon it.\n",
    "\n",
    "TODO: `todo_submittedByProviderId` is present for registry-submitted entries, may allow recovering information about who submitted an entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c52d7-ac3c-4a7f-b3b0-cc94e4b071b2",
   "metadata": {},
   "source": [
    "### Transform: transform_assessment_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb9495-7e17-446e-a4d1-312b5fc79193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_assessment_log(\n",
    "    df_documents: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    # Pull each value of the gad-7 assessment scale out to its own column.\n",
    "    def _transform_gad7_points(\n",
    "        df_documents: pd.DataFrame,\n",
    "    ) -> pd.DataFrame:\n",
    "        def _factory_transform_gad7_points_key(keyJson):\n",
    "            def _transform_gad7_points_key(row):\n",
    "                if row[\"_type\"] != \"assessmentLog\":\n",
    "                    return None\n",
    "                if row[\"assessmentId\"] != \"gad-7\":\n",
    "                    return None\n",
    "                if not row[\"pointValues\"]:\n",
    "                    return None\n",
    "\n",
    "                return row[\"pointValues\"][keyJson]\n",
    "\n",
    "            return _transform_gad7_points_key\n",
    "\n",
    "        gad7EnumMap = {\n",
    "            \"Anxious\": \"gad7Anxious\",\n",
    "            \"Constant worrying\": \"gad7ConstantWorrying\",\n",
    "            \"Worrying too much\": \"gad7WorryingTooMuch\",\n",
    "            \"Trouble relaxing\": \"gad7TroubleRelaxing\",\n",
    "            \"Restless\": \"gad7Restless\",\n",
    "            \"Irritable\": \"gad7Irritable\",\n",
    "            \"Afraid\": \"gad7Afraid\",\n",
    "        }\n",
    "\n",
    "        for (keyJson, keyExport) in gad7EnumMap.items():\n",
    "            df_documents[keyExport] = df_documents.apply(\n",
    "                _factory_transform_gad7_points_key(keyJson), axis=1\n",
    "            )\n",
    "\n",
    "        return df_documents\n",
    "\n",
    "    # Obtain or calculate a gad-7 score.\n",
    "    def _transform_gad7_score(row):\n",
    "        if row[\"_type\"] != \"assessmentLog\":\n",
    "            return None\n",
    "        if row[\"assessmentId\"] != \"gad-7\":\n",
    "            return None\n",
    "\n",
    "        # Some rows already provide a totalScore.\n",
    "        if \"totalScore\" in row and not pd.isna(row[\"totalScore\"]):\n",
    "            return row[\"totalScore\"]\n",
    "\n",
    "        # Otherwise we need to sum the pointValues.\n",
    "        if \"pointValues\" in row and row[\"pointValues\"]:\n",
    "            return sum(row[\"pointValues\"].values())\n",
    "\n",
    "        # We should always have one or the other.\n",
    "        raise ValueError()\n",
    "\n",
    "    # Pull each value of the phq-9 assessment scale out to its own column.\n",
    "    def _transform_phq9_points(\n",
    "        df_documents: pd.DataFrame,\n",
    "    ) -> pd.DataFrame:\n",
    "        def _factory_transform_phq9_points_key(keyJson):\n",
    "            def _transform_phq9_points_key(row):\n",
    "                if row[\"_type\"] != \"assessmentLog\":\n",
    "                    return None\n",
    "                if row[\"assessmentId\"] != \"phq-9\":\n",
    "                    return None\n",
    "                if not row[\"pointValues\"]:\n",
    "                    return None\n",
    "\n",
    "                return row[\"pointValues\"][keyJson]\n",
    "\n",
    "            return _transform_phq9_points_key\n",
    "\n",
    "        phq9EnumMap = {\n",
    "            \"Interest\": \"phq9Interest\",\n",
    "            \"Mood\": \"phq9Mood\",\n",
    "            \"Sleep\": \"phq9Sleep\",\n",
    "            \"Energy\": \"phq9Energy\",\n",
    "            \"Appetite\": \"phq9Appetite\",\n",
    "            \"Guilt\": \"phq9Guilt\",\n",
    "            \"Concentrating\": \"phq9Concentrating\",\n",
    "            \"Motor\": \"phq9Motor\",\n",
    "            \"Suicide\": \"phq9Suicide\",\n",
    "        }\n",
    "\n",
    "        for (keyJson, keyExport) in phq9EnumMap.items():\n",
    "            df_documents[keyExport] = df_documents.apply(\n",
    "                _factory_transform_phq9_points_key(keyJson), axis=1\n",
    "            )\n",
    "\n",
    "        return df_documents\n",
    "\n",
    "    # Obtain or calculate a phq-9 score.\n",
    "    def _transform_phq9_score(row):\n",
    "        if row[\"_type\"] != \"assessmentLog\":\n",
    "            return None\n",
    "        if row[\"assessmentId\"] != \"phq-9\":\n",
    "            return None\n",
    "\n",
    "        # Some rows already provide a totalScore.\n",
    "        if \"totalScore\" in row and not pd.isna(row[\"totalScore\"]):\n",
    "            return row[\"totalScore\"]\n",
    "\n",
    "        # Otherwise we need to sum the pointValues.\n",
    "        if \"pointValues\" in row and row[\"pointValues\"]:\n",
    "            return sum(row[\"pointValues\"].values())\n",
    "\n",
    "        # We should always have one or the other.\n",
    "        raise ValueError()\n",
    "\n",
    "    # Format a recordedDate.\n",
    "    def _transform_recorded_date(row):\n",
    "        if row[\"_type\"] != \"assessmentLog\":\n",
    "            return None\n",
    "\n",
    "        datetime_parsed = date_utils.parse_datetime(datetime=row[\"recordedDateTime\"])\n",
    "        datetime_pacific = datetime_parsed.astimezone(\n",
    "            pytz.timezone(\"America/Los_Angeles\")\n",
    "        )\n",
    "\n",
    "        return datetime_pacific.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def _transform_scheduled_assessment_id(row):\n",
    "        if row[\"_type\"] != \"assessmentLog\":\n",
    "            return None\n",
    "\n",
    "        # Logs labeled on-demand should all be submitted by a social worker.\n",
    "        if row[\"scheduledAssessmentId\"] == \"on-demand\":\n",
    "            if row[\"patientSubmitted\"] == True:\n",
    "                raise ValueError()\n",
    "\n",
    "            return None\n",
    "\n",
    "        return row[\"scheduledAssessmentId\"]\n",
    "\n",
    "    # Format a submittedBy column as requested.\n",
    "    def _transform_submitted_by(row):\n",
    "        if row[\"_type\"] != \"assessmentLog\":\n",
    "            return None\n",
    "\n",
    "        if row[\"patientSubmitted\"] == True:\n",
    "            return \"Pt\"\n",
    "        elif row[\"patientSubmitted\"] == False:\n",
    "            return \"SW\"\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "    df_documents = df_documents.copy()\n",
    "    df_documents = _transform_gad7_points(df_documents)\n",
    "    df_documents[\"gad7Score\"] = df_documents.apply(_transform_gad7_score, axis=1)\n",
    "    df_documents = _transform_phq9_points(df_documents)\n",
    "    df_documents[\"phq9Score\"] = df_documents.apply(_transform_phq9_score, axis=1)\n",
    "    df_documents[\"assessmentLogRecordedDate\"] = df_documents.apply(\n",
    "        _transform_recorded_date, axis=1\n",
    "    )\n",
    "    df_documents[\"assessmentLogScheduledAssessmentId\"] = df_documents.apply(\n",
    "        _transform_scheduled_assessment_id, axis=1\n",
    "    )\n",
    "    df_documents[\"assessmentLogSubmittedBy\"] = df_documents.apply(\n",
    "        _transform_submitted_by, axis=1\n",
    "    )\n",
    "\n",
    "    return df_documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb6ad0ae-e2ec-475c-a8db-d19f1b4177e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Documentation: Mood Logs\n",
    "\n",
    "Exported from `moodLog` documents. A single row is included for each `moodLog`.\n",
    "\n",
    "Includes common fields documented in `commonFields.md`.\n",
    "\n",
    "Notable values:\n",
    "\n",
    "- All values of `_rev` are `1`, as it was not possible to edit a mood log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81349b-4394-4d7f-a7c8-b292f875d3a4",
   "metadata": {},
   "source": [
    "### Transform: transform_mood_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0ea13-58be-4681-a272-8027271a3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mood_log(\n",
    "    df_documents: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    # No transformations are needed.\n",
    "    return df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f3bf5-d492-4353-a4ff-1f77bb1cbe8f",
   "metadata": {},
   "source": [
    "### Utility: apply_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5558e07-0fab-4650-acf5-c7007b762e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(\n",
    "    df_documents: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    df_documents = transform_assessment_log(\n",
    "        df_documents,\n",
    "    )\n",
    "    df_documents = transform_mood_log(\n",
    "        df_documents,\n",
    "    )\n",
    "\n",
    "    return df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dbb08-7ded-42e8-ae73-f7311bd1d217",
   "metadata": {},
   "source": [
    "### Data: patient_id_to_df_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9d127-5d8e-43df-a55b-b2907893927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transform_patient_documents():\n",
    "    patient_id_to_df_documents = {}\n",
    "\n",
    "    progress_max = len(patient_id_to_df_documents_raw)\n",
    "    progress_patient_count = ipywidgets.IntProgress(min=0, max=progress_max)\n",
    "\n",
    "    print(\"Transforming DataFrame for {} Patients\".format(progress_max))\n",
    "    IPython.display.display(progress_patient_count)\n",
    "\n",
    "    progress_patient_count.description = \"{}/{}\".format(0, progress_max)\n",
    "    for patient_count, (patient_id_current, df_documents_raw_current) in enumerate(\n",
    "        patient_id_to_df_documents_raw.items()\n",
    "    ):\n",
    "        patient_id_to_df_documents[patient_id_current] = apply_transforms(\n",
    "            df_documents_raw_current.copy(),\n",
    "        )\n",
    "\n",
    "        progress_patient_count.description = \"{}/{}\".format(\n",
    "            patient_count + 1, progress_max\n",
    "        )\n",
    "        progress_patient_count.value = patient_count + 1\n",
    "\n",
    "    return patient_id_to_df_documents\n",
    "\n",
    "\n",
    "patient_id_to_df_documents = prepare_transform_patient_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8cb36-0220-47b4-b57b-a7a791b3b465",
   "metadata": {},
   "source": [
    "### Prepare Combined Documents DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dcb54-9bac-44a1-a176-02ee7c7b8756",
   "metadata": {},
   "source": [
    "#### Data: df_documents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a807d-8b1c-42ab-91cb-e0cea3b248e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documents_raw = pd.concat(patient_id_to_df_documents_raw.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59ea71-9392-42df-997c-8a3fe970ddae",
   "metadata": {},
   "source": [
    "#### Data: df_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dce5a-e255-461e-9e22-f30700421863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documents = pd.concat(patient_id_to_df_documents.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb3d9a0-e82a-4ec6-a3f4-c794317d0251",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9dcfc-4682-4f23-ad20-ccdd91e38fb9",
   "metadata": {},
   "source": [
    "### Reset Export File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba90b7e-cb19-495b-8c80-57b12fc8c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_file_list: List[ExportFile] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279b467-9bf2-4c93-b53a-6b03fe7a258c",
   "metadata": {},
   "source": [
    "### Documentation: Export\n",
    "\n",
    "This folder contains an export of SCOPE platform data.\n",
    "This includes patient data, should always be stored in an encrypted format, and should be treated as highly sensitive.\n",
    "Tables are commonly exported in both Excel and comma-separated formats\n",
    "(i.e., `.xlsx` via `pd.DataFrame.to_excel`, `.csv` via `pd.DataFrame.to_csv`).\n",
    "Excel will typically be easier to visually inspect,\n",
    "while comma-separated may be easier for R-based analyses.\n",
    "\n",
    "The root folder contains exports that are intended to be used in analyses.\n",
    "\n",
    "- `patients` is a list of all patients included in the export. Documentation in `patients.md`.\n",
    "\n",
    "- `assessments.gad7` is an export of all GAD-7 assessments. Documentation in `assessments.md`.\n",
    "- `assessments.phq9` is an export of all PHQ-9 assessments. Documentation in `assessments.md`.\n",
    "\n",
    "- `moodLogs.phq9` is an export of all mood logs. Documentation in `moodLogs.md`.\n",
    "\n",
    "The `data` folder contains additional exports, intended to support inspection of and communication around underlying data.\n",
    "Exports are prepared in a multi-step process, converting a database of JSON documents into tables for analyses.\n",
    "\n",
    "- Note that document tables are sparse, as most documents (i.e., rows) do not contain most values (i.e., columns).\n",
    "  Each row includes a `_type` taken from the JSON document, commonly used for filtering to specific types of documents.\n",
    "\n",
    "- Raw JSON documents are first loaded into a table for each patient.\n",
    "  This is the raw underlying data, included in the export to allow inspection.\n",
    "  The `data/patients` folder includes a folder for each patient, named according to the `patientId`.  \n",
    "\n",
    "- Tables for all patients are combined in a single large table, exported as `documents.raw`.\n",
    "  This is all of the raw data and contains everything that is available.\n",
    "\n",
    "- A series of transformations are applied to the single large table.\n",
    "  Each transformation computes one or more new columns that are added to the single large table.\n",
    "\n",
    "- After all transformations are completed, the single large table is exported as `documents.transformed`.\n",
    "  Filtering the single large table by `patientId`, transformed tables are also exported to the each folder in `data/patients`.\n",
    "  These are included in the export to allow inspection.\n",
    "\n",
    "- Tables intended for analyses are then exported as different subsets of `documents.transformed`.\n",
    "\n",
    "The `config` folder contains additional configuration that was used in the export.\n",
    "If a change in configuration is needed, these files should be modified and then used in a new export.\n",
    "\n",
    "- `archive_mrn_to_record_id.xlsx` contains the mapping from `MRN` to `recordId` that was used in this export.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42cd64-be60-45e8-a69c-36b981be8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_markdown(pathlib.Path(\"documentation\"), documentation_as_markdown(\"Export\"))\n",
    "export_markdown(\n",
    "    pathlib.Path(\"commonFields\"), documentation_as_markdown(\"Common Fields\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c852677-e238-4229-b02e-3ec8af7c3534",
   "metadata": {},
   "source": [
    "### Patients\n",
    "\n",
    "- Documented above in \"Documentation: Patients Export\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d5e1e-f15d-4533-ba1c-78eb70d9f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_markdown(\n",
    "    pathlib.Path(\"patients\"),\n",
    "    documentation_as_markdown(\"Patients\"),\n",
    ")\n",
    "\n",
    "export_file_bytes(\n",
    "    pathlib.Path(\n",
    "        \"config\",\n",
    "        \"archive_mrn_to_record_id.xlsx\",\n",
    "    ),\n",
    "    mrn_to_record_id_bytes,\n",
    ")\n",
    "\n",
    "export_dataframe(\n",
    "    pathlib.Path(\n",
    "        \"data\",\n",
    "        \"patients.raw\",\n",
    "    ),\n",
    "    df_patients_raw,\n",
    ")\n",
    "\n",
    "export_dataframe(\n",
    "    pathlib.Path(\n",
    "        \"patients\",\n",
    "    ),\n",
    "    dataframe_format_export(\n",
    "        df_patients,\n",
    "        drop_columns=[\"collection\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83807b0b-761c-451d-9890-44bbca8f7941",
   "metadata": {},
   "source": [
    "### Per-Patient Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25a6fe-3b81-48b0-ac6f-5d69f6b174e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_per_patient_documents():\n",
    "    progress_max = len(df_patients)\n",
    "    progress_patient_count = ipywidgets.IntProgress(min=0, max=progress_max)\n",
    "\n",
    "    print(\"Per-Patient Export for {} Patients\".format(progress_max))\n",
    "    IPython.display.display(progress_patient_count)\n",
    "\n",
    "    progress_patient_count.description = \"{}/{}\".format(0, progress_max)\n",
    "    for patient_count, (row_current, patient_current) in enumerate(\n",
    "        df_patients.iterrows()\n",
    "    ):\n",
    "        patient_id_current = patient_current[\"patientId\"]\n",
    "\n",
    "        df_documents_raw_current = patient_id_to_df_documents_raw[patient_id_current]\n",
    "        export_dataframe(\n",
    "            pathlib.Path(\n",
    "                \"data\",\n",
    "                \"patients\",\n",
    "                \"patient_{}\".format(patient_id_current),\n",
    "                \"patient_{}.raw\".format(patient_id_current),\n",
    "            ),\n",
    "            df_documents_raw_current,\n",
    "        )\n",
    "\n",
    "        df_documents_current = patient_id_to_df_documents[patient_id_current]\n",
    "        export_dataframe(\n",
    "            pathlib.Path(\n",
    "                \"data\",\n",
    "                \"patients\",\n",
    "                \"patient_{}\".format(patient_id_current),\n",
    "                \"patient_{}.transformed\".format(patient_id_current),\n",
    "            ),\n",
    "            df_documents_current,\n",
    "        )\n",
    "\n",
    "        progress_patient_count.description = \"{}/{}\".format(\n",
    "            patient_count + 1, progress_max\n",
    "        )\n",
    "        progress_patient_count.value = patient_count + 1\n",
    "\n",
    "\n",
    "if DEVELOPMENT_EXPORT_PER_PATIENT_DOCUMENTS:\n",
    "    export_per_patient_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892c73c-72a6-43b5-be63-97aa530cb952",
   "metadata": {},
   "source": [
    "### Combined Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce22c52-2af0-4003-b253-d0076c829797",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVELOPMENT_EXPORT_COMBINED_DOCUMENTS:\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"documents.raw\",\n",
    "        ),\n",
    "        df_documents_raw,\n",
    "    )\n",
    "\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"documents.transformed\",\n",
    "        ),\n",
    "        df_documents,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b86e1-e598-457a-ae69-43a6ab30c1c8",
   "metadata": {},
   "source": [
    "### Analysis: Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd6967-edec-4d4a-b1f2-747d5b33d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_assessments():\n",
    "    # Documentation of this analysis.\n",
    "    export_markdown(\n",
    "        pathlib.Path(\"assessments\"),\n",
    "        documentation_as_markdown(\"Assessments\"),\n",
    "    )\n",
    "\n",
    "    # Preliminary GAD-7 documents.\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"assessments.gad7.raw\",\n",
    "        ),\n",
    "        dataframe_format_export(\n",
    "            df_documents_raw[\n",
    "                (df_documents_raw[\"_type\"] == \"assessmentLog\")\n",
    "                & (df_documents_raw[\"assessmentId\"] == \"gad-7\")\n",
    "            ],\n",
    "            drop_empty_columns=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"assessments.gad7.transformed\",\n",
    "        ),\n",
    "        dataframe_format_export(\n",
    "            df_documents[\n",
    "                (df_documents[\"_type\"] == \"assessmentLog\")\n",
    "                & (df_documents[\"assessmentId\"] == \"gad-7\")\n",
    "            ],\n",
    "            drop_empty_columns=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Preliminary PHQ-9 documents.\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"assessments.phq9.raw\",\n",
    "        ),\n",
    "        dataframe_format_export(\n",
    "            df_documents_raw[\n",
    "                (df_documents_raw[\"_type\"] == \"assessmentLog\")\n",
    "                & (df_documents_raw[\"assessmentId\"] == \"phq-9\")\n",
    "            ],\n",
    "            drop_empty_columns=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"assessments.phq9.transformed\",\n",
    "        ),\n",
    "        dataframe_format_export(\n",
    "            df_documents[\n",
    "                (df_documents[\"_type\"] == \"assessmentLog\")\n",
    "                & (df_documents[\"assessmentId\"] == \"phq-9\")\n",
    "            ],\n",
    "            drop_empty_columns=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Formatted GAD-7 and PHQ-9 documents.\n",
    "    drop_columns = [\n",
    "        \"_type\",\n",
    "        \"_set_id\",\n",
    "        \"patientSubmitted\",\n",
    "        \"pointValues\",\n",
    "        \"recordedDateTime\",\n",
    "        \"scheduledAssessmentId\",\n",
    "        \"totalScore\",\n",
    "    ]\n",
    "    rename_columns = {\n",
    "        \"_id\": \"_docId\",\n",
    "        \"assessmentLogId\": \"_assessmentLogId\",\n",
    "        \"assessmentLogRecordedDate\": \"recordedDate\",\n",
    "        \"assessmentLogScheduledAssessmentId\": \"todo_scheduledAssessmentId\",\n",
    "        \"submittedByProviderId\": \"todo_submittedByProviderId\",\n",
    "        \"assessmentLogSubmittedBy\": \"submittedBy\",\n",
    "    }\n",
    "    sort_columns = [\n",
    "        \"recordId\",\n",
    "        \"_patientId\",\n",
    "        \"_docId\",\n",
    "        \"_assessmentLogId\",\n",
    "        \"_rev\",\n",
    "        \"_created\",\n",
    "        \"assessmentId\",\n",
    "        \"recordedDate\",\n",
    "        \"submittedBy\",\n",
    "        \"todo_scheduledAssessmentId\",\n",
    "        \"todo_submittedByProviderId\",\n",
    "        \"gad7Anxious\",\n",
    "        \"gad7ConstantWorrying\",\n",
    "        \"gad7WorryingTooMuch\",\n",
    "        \"gad7TroubleRelaxing\",\n",
    "        \"gad7Restless\",\n",
    "        \"gad7Irritable\",\n",
    "        \"gad7Afraid\",\n",
    "        \"gad7Score\",\n",
    "        \"phq9Interest\",\n",
    "        \"phq9Mood\",\n",
    "        \"phq9Sleep\",\n",
    "        \"phq9Energy\",\n",
    "        \"phq9Appetite\",\n",
    "        \"phq9Guilt\",\n",
    "        \"phq9Concentrating\",\n",
    "        \"phq9Motor\",\n",
    "        \"phq9Suicide\",\n",
    "        \"phq9Score\",\n",
    "        \"comment\",\n",
    "    ]\n",
    "    sort_rows_by_columns = [\n",
    "        \"recordId\",\n",
    "        \"_patientId\",\n",
    "        \"_assessmentLogId\",\n",
    "        \"_rev\",\n",
    "    ]\n",
    "\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\"assessments.gad7\"),\n",
    "        dataframe_format_export(\n",
    "            df_documents[\n",
    "                (df_documents[\"_type\"] == \"assessmentLog\")\n",
    "                & (df_documents[\"assessmentId\"] == \"gad-7\")\n",
    "            ],\n",
    "            drop_empty_columns=True,\n",
    "            drop_columns=drop_columns,\n",
    "            rename_columns=rename_columns,\n",
    "            sort_columns=sort_columns,\n",
    "            sort_rows_by_columns=sort_rows_by_columns,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\"assessments.phq9\"),\n",
    "        dataframe_format_export(\n",
    "            df_documents[\n",
    "                (df_documents[\"_type\"] == \"assessmentLog\")\n",
    "                & (df_documents[\"assessmentId\"] == \"phq-9\")\n",
    "            ],\n",
    "            drop_empty_columns=True,\n",
    "            drop_columns=drop_columns,\n",
    "            rename_columns=rename_columns,\n",
    "            sort_columns=sort_columns,\n",
    "            sort_rows_by_columns=sort_rows_by_columns,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "export_analysis_assessments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ceac32-4efc-450b-a7b4-de9ca1628a8b",
   "metadata": {},
   "source": [
    "### Analysis: Mood Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8248e1-d54f-41a9-8199-1203af56f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_mood_logs():\n",
    "    # Documentation of this analysis.\n",
    "    export_markdown(\n",
    "        pathlib.Path(\"moodLogs\"),\n",
    "        documentation_as_markdown(\"Mood Logs\"),\n",
    "    )\n",
    "\n",
    "    # Preliminary documents.\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"moodLogs.raw\",\n",
    "        ),\n",
    "        dataframe_format_export(\n",
    "            df_documents_raw[(df_documents_raw[\"_type\"] == \"moodLog\")],\n",
    "            drop_empty_columns=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\n",
    "            \"data\",\n",
    "            \"moodLogs.transformed\",\n",
    "        ),\n",
    "        dataframe_format_export(\n",
    "            df_documents[(df_documents_raw[\"_type\"] == \"moodLog\")],\n",
    "            drop_empty_columns=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Formatted mood logs.\n",
    "    drop_columns = [\n",
    "        \"_type\",\n",
    "        \"_set_id\",\n",
    "        \"recordedDateTime\",\n",
    "    ]\n",
    "    rename_columns = {\n",
    "        \"_id\": \"_docId\",\n",
    "        \"moodLogId\": \"_moodLogId\",\n",
    "    }\n",
    "    sort_columns = [\n",
    "        \"recordId\",\n",
    "        \"_patientId\",\n",
    "        \"_docId\",\n",
    "        \"_moodLogId\",\n",
    "        \"_rev\",\n",
    "        \"_created\",\n",
    "        \"mood\",\n",
    "        \"comment\",\n",
    "    ]\n",
    "    sort_rows_by_columns = [\n",
    "        \"recordId\",\n",
    "        \"_patientId\",\n",
    "        \"_created\",\n",
    "    ]\n",
    "\n",
    "    export_dataframe(\n",
    "        pathlib.Path(\"moodLogs\"),\n",
    "        dataframe_format_export(\n",
    "            df_documents[(df_documents[\"_type\"] == \"moodLog\")],\n",
    "            drop_empty_columns=True,\n",
    "            drop_columns=drop_columns,\n",
    "            rename_columns=rename_columns,\n",
    "            sort_columns=sort_columns,\n",
    "            sort_rows_by_columns=sort_rows_by_columns,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "export_analysis_mood_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d4111-7faf-42ec-ba24-d65b9285810a",
   "metadata": {},
   "source": [
    "### Write Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e25a5-a095-42a2-bfc8-7f3de4ddb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The export is stored in a single zip file.\n",
    "with open(\n",
    "    pathlib.Path(\n",
    "        archive_dir_path,\n",
    "        \"export_{}.zip\".format(archive_suffix),\n",
    "    ),\n",
    "    mode=\"xb\",\n",
    ") as archive_file:\n",
    "    with pyzipper.AESZipFile(\n",
    "        archive_file,\n",
    "        \"w\",\n",
    "        compression=pyzipper.ZIP_LZMA,\n",
    "        encryption=pyzipper.WZ_AES,\n",
    "    ) as archive_zipfile:\n",
    "        # Set the password\n",
    "        archive_zipfile.setpassword(archive_password.encode(\"utf-8\"))\n",
    "\n",
    "        for file_current in export_file_list:\n",
    "            if file_current.type in [\n",
    "                ExportFileType.BYTES,\n",
    "                ExportFileType.EXCEL,\n",
    "            ]:\n",
    "                archive_zipfile.writestr(\n",
    "                    file_current.path.as_posix(), file_current.bytes\n",
    "                )\n",
    "            elif file_current.type in [\n",
    "                ExportFileType.CSV,\n",
    "                ExportFileType.MARKDOWN,\n",
    "            ]:\n",
    "                archive_zipfile.writestr(\n",
    "                    file_current.path.as_posix(), file_current.text.encode(\"utf-8\")\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Unknown ExportFileType\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
